"""
YOLO11 Fine-Tuning Script - Optimized for NVIDIA A100 40GB GPU

Fine-tune YOLO11 (latest Ultralytics) for interview-specific object detection:
- Phones (handheld, on desk)
- Earbuds/Headphones
- Multiple people  
- Cheat sheets / notes
- Secondary screens

Optimizations for A100 40GB:
- Batch sizes optimized for 40GB VRAM
- Mixed precision (FP16/BF16) support
- Multi-threaded data loading
- RAM caching with 60GB available

Krutrim Cloud: A100-NVLINK-Standard-1x @ â‚¹170/hour
- 40GB GPU Memory
- 60GB RAM  
- 16 vCPUs

https://docs.ultralytics.com/models/yolo11/
"""

from __future__ import annotations

import argparse
import os
from pathlib import Path
from datetime import datetime
from typing import Optional

import torch
from ultralytics import YOLO


# A100 40GB optimized configurations
A100_40GB_CONFIGS = {
    # Fast training - for quick experiments
    "fast": {
        "model": "yolo11s.pt",
        "epochs": 100,
        "batch": 32,          # A100 40GB handles this well
        "imgsz": 640,
        "patience": 20,
    },
    # Balanced - good accuracy with reasonable training time
    "balanced": {
        "model": "yolo11m.pt",
        "epochs": 150,
        "batch": 16,
        "imgsz": 640,
        "patience": 30,
    },
    # Accurate - high accuracy
    "accurate": {
        "model": "yolo11l.pt",
        "epochs": 250,
        "batch": 12,
        "imgsz": 800,
        "patience": 50,
    },
    # Best - very high accuracy
    "best": {
        "model": "yolo11x.pt",
        "epochs": 300,
        "batch": 8,
        "imgsz": 800,
        "patience": 60,
    },
    # Ultimate - MAXIMUM accuracy with YOLO11x (default)
    "ultimate": {
        "model": "yolo11x.pt",
        "epochs": 400,
        "batch": 6,           # A100 40GB with imgsz=1024
        "imgsz": 1024,
        "patience": 80,
    },
}


def check_gpu():
    """Check GPU availability and memory."""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"âœ… GPU detected: {gpu_name}")
        print(f"   Memory: {gpu_mem:.1f} GB")
        
        if "A100" in gpu_name:
            if gpu_mem < 50:
                print("   âš¡ A100 40GB detected - using optimized settings!")
            else:
                print("   âš¡ A100 80GB detected - you can use larger batches!")
        elif "H100" in gpu_name:
            print("   ðŸš€ H100 detected - consider using train_yolo_h100.py!")
        return True
    else:
        print("âš ï¸  No CUDA GPU detected - training will be slow on CPU")
        return False


def get_optimal_workers():
    """Get optimal number of dataloader workers for A100 40GB."""
    cpu_count = os.cpu_count() or 8
    # A100 40GB instance has 16 vCPUs
    return min(cpu_count, 12)


def create_dataset_yaml(data_dir: Path, output_path: Path):
    """Create dataset YAML configuration."""
    yaml_content = f"""
# AI Interview Proctoring Dataset
# Generated by train_yolo_a100_40gb.py
path: {data_dir.absolute()}
train: train/images
val: valid/images
test: test/images

# 16 Unified Classes for AI Interview Monitoring
names:
  0: phone
  1: earbuds
  2: smartwatch
  3: notes
  4: another_person
  5: laptop
  6: second_screen
  7: calculator
  8: pen
  9: looking_away
  10: looking_forward
  11: peeking
  12: talking
  13: hand_gesture
  14: normal
  15: cheating

nc: 16
"""
    output_path.write_text(yaml_content)
    print(f"âœ… Dataset YAML created: {output_path}")
    return output_path


def train(
    data_yaml: str,
    base_model: str = "yolo11x.pt",
    epochs: int = 400,
    imgsz: int = 1024,
    batch: int = 6,
    patience: int = 80,
    project: str = "runs/detect",
    name: Optional[str] = None,
    device: str = "0",
    resume: bool = False,
    preset: Optional[str] = None,
):
    """
    Fine-tune YOLO11x on custom dataset - Optimized for A100 40GB GPU.
    
    Args:
        data_yaml: Path to dataset YAML file
        base_model: Base model (default: yolo11x - ultimate accuracy)
        epochs: Number of training epochs (default: 400 for maximum accuracy)
        imgsz: Image size for training (default: 1024 for best detection)
        batch: Batch size (default: 6 for A100 40GB)
        patience: Early stopping patience (default: 80 for thorough training)
        project: Project directory for outputs
        name: Run name (auto-generated if None)
        device: Device to use (0 for GPU)
        resume: Resume from last checkpoint
        preset: Training preset (fast, balanced, accurate, best, ultimate)
    """
    # Check GPU
    has_gpu = check_gpu()
    
    # Apply preset if specified
    if preset and preset in A100_40GB_CONFIGS:
        config = A100_40GB_CONFIGS[preset]
        base_model = config["model"]
        epochs = config["epochs"]
        batch = config["batch"]
        imgsz = config["imgsz"]
        patience = config["patience"]
        print(f"\nðŸ“‹ Using '{preset}' preset for A100 40GB")
    
    # Get optimal workers
    workers = get_optimal_workers()
    
    print(f"\nðŸš€ Starting YOLO11 fine-tuning (A100 40GB Optimized)...")
    print(f"   Base model: {base_model}")
    print(f"   Dataset: {data_yaml}")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch}")
    print(f"   Image size: {imgsz}")
    print(f"   Workers: {workers}")
    print(f"   Mixed Precision (AMP): Enabled")
    print(f"   Device: cuda:{device}")
    
    # Load base model
    model = YOLO(base_model)
    
    # Generate run name if not provided
    if name is None:
        name = f"interview_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # A100 40GB optimized training configuration
    results = model.train(
        data=data_yaml,
        epochs=epochs,
        imgsz=imgsz,
        batch=batch,
        patience=patience,
        project=project,
        name=name,
        device=device if has_gpu else "cpu",
        resume=resume,
        
        # A100 40GB Performance Optimization
        amp=True,              # Mixed precision - BF16/FP16 on A100
        cache="ram",           # Cache in RAM - 60GB available
        workers=workers,       # Optimal workers for 16 vCPUs
        
        # Strong augmentation for better generalization & accuracy
        hsv_h=0.015,           # Hue augmentation
        hsv_s=0.7,             # Saturation augmentation
        hsv_v=0.4,             # Value augmentation
        degrees=15.0,          # Increased rotation
        translate=0.15,        # More translation
        scale=0.5,             # More scale variation
        shear=2.0,             # Add shear
        perspective=0.0001,    # Slight perspective
        flipud=0.0,            # No vertical flip
        fliplr=0.5,            # Horizontal flip
        mosaic=1.0,            # Full mosaic augmentation
        mixup=0.2,             # Increased mixup
        copy_paste=0.15,       # More copy-paste augmentation
        erasing=0.3,           # Random erasing for robustness
        
        # Optimization - tuned for accuracy
        optimizer="AdamW",
        lr0=0.0008,            # Slightly lower LR for stability
        lrf=0.01,
        momentum=0.937,
        weight_decay=0.0005,
        warmup_epochs=5,       # Longer warmup
        warmup_momentum=0.8,
        cos_lr=True,           # Cosine LR scheduler
        
        # Training stability & accuracy
        nbs=64,                # Nominal batch size for loss scaling
        close_mosaic=15,       # Disable mosaic for last 15 epochs
        
        # Multi-scale training for better accuracy
        rect=False,            # Disable rect training for better augmentation
        
        # Output
        save=True,
        save_period=10,
        plots=True,
        verbose=True,
        exist_ok=True,
    )
    
    print(f"\nâœ… Training complete!")
    print(f"   Best model: {project}/{name}/weights/best.pt")
    print(f"   Last model: {project}/{name}/weights/last.pt")
    
    # Print final metrics
    if results:
        print(f"\nðŸ“ˆ Training Results:")
        try:
            print(f"   mAP50: {results.results_dict.get('metrics/mAP50(B)', 'N/A'):.4f}")
            print(f"   mAP50-95: {results.results_dict.get('metrics/mAP50-95(B)', 'N/A'):.4f}")
        except:
            print("   Check results in the output folder")
    
    return results


def export_model(model_path: str, format: str = "onnx"):
    """Export trained model to different format."""
    model = YOLO(model_path)
    model.export(format=format)
    print(f"âœ… Model exported to {format}")


def main():
    parser = argparse.ArgumentParser(
        description="Fine-tune YOLO11x for interview detection (A100 40GB Optimized)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # RECOMMENDED: Ultimate accuracy with YOLO11x (default)
    python scripts/train_yolo_a100_40gb.py --data data.yaml
    
    # Or explicitly use ultimate preset
    python scripts/train_yolo_a100_40gb.py --data data.yaml --preset ultimate
    
    # Faster training if needed
    python scripts/train_yolo_a100_40gb.py --data data.yaml --preset balanced

Presets (optimized for A100 40GB @ â‚¹170/hour):
    fast      - yolo11s, batch=32, 100 epochs  (~20-30 min)   â‚¹85
    balanced  - yolo11m, batch=16, 150 epochs  (~45-60 min)   â‚¹170
    accurate  - yolo11l, batch=12, 250 epochs  (~1.5-2 hrs)   â‚¹340
    best      - yolo11x, batch=8,  300 epochs  (~2.5-3 hrs)   â‚¹510
    ultimate  - yolo11x, batch=6,  400 epochs  (~3.5-4 hrs)   â‚¹680 [DEFAULT]

Note: A100 40GB is â‚¹170/hr vs H100 at â‚¹213/hr
      But H100 is ~1.5x faster, so total cost may be similar.
        """
    )
    
    parser.add_argument("--data", type=str, required=True, help="Path to dataset YAML")
    parser.add_argument("--model", type=str, default="yolo11x.pt", help="Base model (yolo11s, yolo11m, yolo11l, yolo11x)")
    parser.add_argument("--preset", type=str, choices=["fast", "balanced", "accurate", "best", "ultimate"], default="ultimate", help="Training preset (default: ultimate)")
    parser.add_argument("--epochs", type=int, default=400, help="Training epochs (default: 400)")
    parser.add_argument("--batch", type=int, default=6, help="Batch size (default: 6 for A100 40GB)")
    parser.add_argument("--imgsz", type=int, default=1024, help="Image size (default: 1024)")
    parser.add_argument("--patience", type=int, default=80, help="Early stopping patience")
    parser.add_argument("--project", type=str, default="runs/detect", help="Project directory")
    parser.add_argument("--name", type=str, default=None, help="Run name")
    parser.add_argument("--device", type=str, default="0", help="Device (0, 1, cpu)")
    parser.add_argument("--resume", action="store_true", help="Resume training")
    parser.add_argument("--export", type=str, default=None, help="Export format after training (onnx, tensorrt)")
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ðŸš€ YOLO11 Training - Optimized for NVIDIA A100 40GB GPU")
    print("   Krutrim Cloud: A100-NVLINK-Standard-1x @ â‚¹170/hour")
    print("=" * 70)
    
    # Validate data path
    if not Path(args.data).exists():
        print(f"âŒ Dataset YAML not found: {args.data}")
        print("\nTo create a dataset, organize your images as:")
        print("  data/")
        print("  â”œâ”€â”€ train/images/")
        print("  â”œâ”€â”€ train/labels/")
        print("  â”œâ”€â”€ valid/images/")
        print("  â”œâ”€â”€ valid/labels/")
        print("  â”œâ”€â”€ test/images/")
        print("  â”œâ”€â”€ test/labels/")
        print("  â””â”€â”€ data.yaml")
        return
    
    # Train
    results = train(
        data_yaml=args.data,
        base_model=args.model,
        epochs=args.epochs,
        batch=args.batch,
        imgsz=args.imgsz,
        patience=args.patience,
        project=args.project,
        name=args.name,
        device=args.device,
        resume=args.resume,
        preset=args.preset,
    )
    
    # Export if requested
    if args.export:
        best_model = Path(args.project) / (args.name or "train") / "weights" / "best.pt"
        if best_model.exists():
            export_model(str(best_model), args.export)


if __name__ == "__main__":
    main()
